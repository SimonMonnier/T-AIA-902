{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "043ab14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d906019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: smonnier. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\smonn\\Desktop\\T-AIA-902-NAN_2\\notebooks\\simon\\wandb\\run-20230630_141818-hnuseefq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/hnuseefq' target=\"_blank\">upbeat-frost-43</a></strong> to <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/hnuseefq' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/hnuseefq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/hnuseefq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x240570d9430>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init Weights and Biases\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"Taxi-Deep-Q-learning\", entity=os.getenv(\"WANDB_LOGIN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48dde93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.envs.toy_text.taxi import TaxiEnv\n",
    "\n",
    "class MultiPassengerTaxiEnv(TaxiEnv):\n",
    "    def __init__(self, num_passengers=4):\n",
    "        super(MultiPassengerTaxiEnv, self).__init__()\n",
    "        self.spec = gym.spec(\"Taxi-v3\")\n",
    "        self.num_passengers = num_passengers\n",
    "        self.action_space = spaces.Discrete(6 + (num_passengers - 1))\n",
    "        self.passenger_status = [0] * num_passengers\n",
    "    \n",
    "    def encode(self, taxi_row, taxi_col, pass_loc, dest_idx):\n",
    "        i = taxi_row\n",
    "        i *= 5\n",
    "        i += taxi_col\n",
    "        i *= 5\n",
    "        i += pass_loc\n",
    "        i *= 4\n",
    "        i += dest_idx\n",
    "        return i\n",
    "\n",
    "    def decode(self, i):\n",
    "        out = []\n",
    "        out.append(i % 4)\n",
    "        i = i // 4\n",
    "        out.append(i % 5)\n",
    "        i = i // 5\n",
    "        out.append(i % 5)\n",
    "        i = i // 5\n",
    "        out.append(i)\n",
    "        return reversed(out)\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if action >= 6:\n",
    "            if self.passenger_status[action - 6] == 1:\n",
    "                self.passenger_status[action - 6] = 2\n",
    "                reward = 20\n",
    "            else:\n",
    "                reward = -10\n",
    "            return self._get_obs(), reward, self._is_done(), {\"prob\": 1.0}\n",
    "        \n",
    "        obs, reward, done, info, _ = super(MultiPassengerTaxiEnv, self).step(action)\n",
    "        taxi_row, taxi_col, pass_loc, dest_idx = self.decode(self.s)\n",
    "        if pass_loc == 4:\n",
    "            self.passenger_status[0] = 2\n",
    "        done = self._is_done()\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def _is_done(self):\n",
    "        return all(status == 2 for status in self.passenger_status)\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        return self.s\n",
    "    \n",
    "    def reset(self):\n",
    "        super(MultiPassengerTaxiEnv, self).reset()\n",
    "        self.passenger_status = [0] * self.num_passengers\n",
    "        return self._get_obs()\n",
    "\n",
    "# Create an environment with 4 passengers\n",
    "env = MultiPassengerTaxiEnv(num_passengers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4cc3def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: 500 11 7\n",
      "Action: 9\n"
     ]
    }
   ],
   "source": [
    "print('Observation:', env.observation_space.n, env.desc.shape[1], env.desc.shape[0])\n",
    "print('Action:', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d26fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        # init dqn layer\n",
    "        self.emb = nn.Embedding(500, 10)\n",
    "        self.fc1 = nn.Linear(in_features=10, out_features=50)\n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=50)\n",
    "        self.out_features = nn.Linear(in_features=50, out_features=env.action_space.n)\n",
    "    \n",
    "    def forward(self, input_t):\n",
    "        #print(input_t)\n",
    "        input_t = nn.functional.relu(self.fc1(self.emb(input_t)))\n",
    "        #print(input_t)\n",
    "        #input_t = nn.functional.relu(self.fc1(input_t))\n",
    "        input_t = nn.functional.relu(self.fc2(input_t))\n",
    "        input_t = self.out_features(input_t)\n",
    "        return input_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1fe13fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', ('state', 'action', 'next_state', 'reward', 'done'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a389ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a271f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "    \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * (current_step) * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf821a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, strategy):\n",
    "        self.current_episode = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        \n",
    "    def get_epsilon(self):\n",
    "        epsilon = self.strategy.get_exploration_rate(self.current_episode)\n",
    "        return epsilon\n",
    "    \n",
    "    def select_action(self, state, policy_network):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_episode)\n",
    "\n",
    "        if rate > random.random():\n",
    "            return random.randrange(self.num_actions) # explore\n",
    "        else:\n",
    "            with torch.no_grad(): # No grad because we use the model to select an action and not for training\n",
    "                predicted = policy_network(torch.tensor([state], device=self.device)) # TODO check diff no tensor\n",
    "                return predicted.argmax(dim=1).item() # exploit    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2be0c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    batch = Experience(*zip(*experiences))\n",
    "    tensor_state = torch.cat(batch.state)\n",
    "    tensor_action = torch.cat(batch.action)\n",
    "    tensor_reward = torch.cat(batch.reward)\n",
    "    tensor_next_state = torch.cat(batch.next_state)\n",
    "    tensor_done = torch.cat(batch.done)\n",
    "    return (tensor_state, tensor_action, tensor_reward, tensor_next_state, tensor_done)\n",
    "\n",
    "#https://www.youtube.com/watch?v=ewRw996uevM&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=18 15.00min\n",
    "class QValues():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(1)) # send all states and actions pairs and get list of qvalues\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_next(target_net, next_states):\n",
    "        values = target_net(next_states).max(dim=1)[0]\n",
    "        return values\n",
    "        \n",
    "def get_game_state(env):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    taxi_row, taxi_col, pass_idx, dest_idx = env.decode(env.s)\n",
    "    rend = env.desc.copy()\n",
    "    if pass_idx < 4:\n",
    "        rend[1 + env.locs[pass_idx][0]][2 * env.locs[pass_idx][1] + 1] = 'P'\n",
    "    rend[1 + env.locs[dest_idx][0]][2 * env.locs[dest_idx][1] + 1] = 'D'\n",
    "    rend[1 + taxi_row][2 * taxi_col + 1] = 'T'\n",
    "    rend = rend.view(np.uint8).astype(np.float32) # Char to float\n",
    "    rend = torch.tensor(rend, device=device) # numpy to tensor\n",
    "    #print(rend.unsqueeze(1))\n",
    "    return rend.unsqueeze(0)\n",
    "\n",
    "def moving_average(x, periods=5):\n",
    "        if len(x) < periods:\n",
    "            return x\n",
    "        cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "        res = (cumsum[periods:] - cumsum[:-periods]) / periods\n",
    "        return np.hstack([x[:periods-1], res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ffec6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:hnuseefq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318e105423df408893ac6d749e3b6f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upbeat-frost-43</strong> at: <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/hnuseefq' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/hnuseefq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230630_141818-hnuseefq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:hnuseefq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39245a5bcb14b3cb180cc7eba263455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\smonn\\Desktop\\T-AIA-902-NAN_2\\notebooks\\simon\\wandb\\run-20230630_141820-9kbeim1e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/9kbeim1e' target=\"_blank\">firm-snowflake-44</a></strong> to <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/9kbeim1e' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/9kbeim1e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'wandb.config = {\\n    \"learning_rate\": 0.001,\\n    \"batch_size\": 128,\\n    \"epsilon\": 1\\n}'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_delay = 0.001\n",
    "target_update = 20 # update target network every 20 episode\n",
    "memory_size = 50000 # TODO try memory size and batch\n",
    "lr_initial = 0.001\n",
    "lr_final = 0.0001\n",
    "num_episodes = 3000\n",
    "gamma = 0.99\n",
    "gamma_lr = (lr_final / lr_initial) ** (1 / num_episodes)\n",
    "\n",
    "run = wandb.init(project=\"Taxi-Deep-Q-learning\", entity=os.getenv(\"WANDB_LOGIN\"))\n",
    "'''wandb.config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 128,\n",
    "    \"epsilon\": 1\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f8f9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_delay)\n",
    "agent = Agent(env, strategy)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "policy_net = DQN(env).to(device)\n",
    "target_net = DQN(env).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval() # eval mode, not training\n",
    "\n",
    "optimizer = torch.optim.Adam(params=policy_net.parameters(), lr=lr_initial)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e98b10ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode: 2999/3000\r"
     ]
    }
   ],
   "source": [
    "max_step = 100\n",
    "all_rewards = []\n",
    "epsilon_vec = []\n",
    "episode_durations = []\n",
    "for episode in range(num_episodes):\n",
    "    #clear_output(wait=True)\n",
    "    print(\"Training episode: {0}/{1}\".format(episode, num_episodes), end=\"\\r\")\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    for step in range(max_step):\n",
    "        # Do action\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        memory.push(Experience(torch.tensor([state], device=device), \n",
    "                               torch.tensor([action], device=device),\n",
    "                               torch.tensor([new_state], device=device),\n",
    "                               torch.tensor([reward], device=device),\n",
    "                               torch.tensor([done], device=device, dtype=torch.bool)))\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "        # Train model\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states_b, actions_b, rewards_b, new_states_b, dones_b = extract_tensors(experiences)\n",
    "            \n",
    "            # predict q values\n",
    "            current_q_values = QValues.get_current(policy_net, states_b, actions_b)\n",
    "            \n",
    "            # expected q values\n",
    "            next_q_values = QValues.get_next(target_net, new_states_b)\n",
    "            \n",
    "            target_q_values = (next_q_values * gamma) + rewards_b # look better ?\n",
    "            \n",
    "            # compute loss\n",
    "            loss = nn.functional.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            \n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad() # prevent accumulating gradients during back props\n",
    "            loss.backward() # compute the gradient of the loss with respect of weight and biases in the policy net\n",
    "            for param in policy_net.parameters(): # Clips gradients computed during backpropagation to avoid explosion of gradients\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            optimizer.step() # update the weight and biases with the just previous gradients computed     \n",
    "        if done:\n",
    "            break\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict()) # update target net with policy net weight\n",
    "        \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    wandb.log({\"reward\": episode_reward, \"duration\": step, \"epsilon\": agent.get_epsilon(), \"learning_rate\": current_lr})\n",
    "    all_rewards.append(episode_reward)\n",
    "    episode_durations.append(step)\n",
    "    epsilon_vec.append(agent.get_epsilon())\n",
    "    agent.current_episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a2919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL (Gym)",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
