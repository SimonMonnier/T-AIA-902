{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139ca77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b2babe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: smonnier. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\smonn\\Desktop\\T-AIA-902-NAN_2\\notebooks\\simon\\wandb\\run-20230630_163527-d2mhsw25</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/d2mhsw25' target=\"_blank\">kind-darkness-57</a></strong> to <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/d2mhsw25' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/d2mhsw25</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/d2mhsw25?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x24b4eb4e4f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init Weights and Biases\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"Taxi-Deep-Q-learning\", entity=os.getenv(\"WANDB_LOGIN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f9c3667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: 500 11 7\n",
      "Action: 6\n"
     ]
    }
   ],
   "source": [
    "env_name = \"Taxi-v3\"\n",
    "env = gym.make(env_name)\n",
    "print('Observation:', env.observation_space.n, env.desc.shape[1], env.desc.shape[0])\n",
    "print('Action:', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4efb0",
   "metadata": {},
   "source": [
    "## Deep QNetwork\n",
    "nn.module = base for all neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40832a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        # init dqn layer\n",
    "        self.emb = nn.Embedding(500, 10)\n",
    "        self.fc1 = nn.Linear(in_features=10, out_features=50)\n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=50)\n",
    "        self.out_features = nn.Linear(in_features=50, out_features=env.action_space.n)\n",
    "    \n",
    "    def forward(self, input_t):\n",
    "        #print(input_t)\n",
    "        input_t = nn.functional.relu(self.fc1(self.emb(input_t)))\n",
    "        #print(input_t)\n",
    "        #input_t = nn.functional.relu(self.fc1(input_t))\n",
    "        input_t = nn.functional.relu(self.fc2(input_t))\n",
    "        input_t = self.out_features(input_t)\n",
    "        return input_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ea4dc",
   "metadata": {},
   "source": [
    "## Experience class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b5c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', ('state', 'action', 'next_state', 'reward', 'done'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba14a40",
   "metadata": {},
   "source": [
    "## Replay Memory class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa0c3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea294113",
   "metadata": {},
   "source": [
    "## Epsilon Greedy Strategy class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4719bbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "    \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * (current_step) * self.decay) # TODO fix it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7e1811",
   "metadata": {},
   "source": [
    "## Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b563b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, strategy): # TODO add device for torch\n",
    "        self.current_episode = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        \n",
    "    def get_epsilon(self):\n",
    "        epsilon = self.strategy.get_exploration_rate(self.current_episode)\n",
    "        return epsilon\n",
    "    \n",
    "    def select_action(self, state, policy_network):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_episode)\n",
    "        #print(rate)\n",
    "        # random.uniform(0, 1)?\n",
    "        if rate > random.random():\n",
    "            return random.randrange(self.num_actions) # explore\n",
    "        else:\n",
    "            with torch.no_grad(): # No grad because we use the model to select an action and not for training\n",
    "                predicted = policy_network(torch.tensor([state], device=self.device)) # TODO check diff no tensor\n",
    "                return predicted.argmax(dim=1).item() # exploit    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e145d2b7",
   "metadata": {},
   "source": [
    "## Usefull function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb639373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    batch = Experience(*zip(*experiences))\n",
    "    tensor_state = torch.cat(batch.state)\n",
    "    tensor_action = torch.cat(batch.action)\n",
    "    tensor_reward = torch.cat(batch.reward)\n",
    "    tensor_next_state = torch.cat(batch.next_state)\n",
    "    tensor_done = torch.cat(batch.done)\n",
    "    return (tensor_state, tensor_action, tensor_reward, tensor_next_state, tensor_done)\n",
    "\n",
    "#https://www.youtube.com/watch?v=ewRw996uevM&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=18 15.00min\n",
    "class QValues():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(1)) # send all states and actions pairs and get list of qvalues\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_next(target_net, next_states):\n",
    "        values = target_net(next_states).max(dim=1)[0]\n",
    "        return values\n",
    "        \n",
    "def get_game_state(env):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    taxi_row, taxi_col, pass_idx, dest_idx = env.decode(env.s)\n",
    "    rend = env.desc.copy()\n",
    "    if pass_idx < 4:\n",
    "        rend[1 + env.locs[pass_idx][0]][2 * env.locs[pass_idx][1] + 1] = 'P'\n",
    "    rend[1 + env.locs[dest_idx][0]][2 * env.locs[dest_idx][1] + 1] = 'D'\n",
    "    rend[1 + taxi_row][2 * taxi_col + 1] = 'T'\n",
    "    rend = rend.view(np.uint8).astype(np.float32) # Char to float\n",
    "    rend = torch.tensor(rend, device=device) # numpy to tensor\n",
    "    #print(rend.unsqueeze(1))\n",
    "    return rend.unsqueeze(0)\n",
    "\n",
    "def moving_average(x, periods=5):\n",
    "        if len(x) < periods:\n",
    "            return x\n",
    "        cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "        res = (cumsum[periods:] - cumsum[:-periods]) / periods\n",
    "        return np.hstack([x[:periods-1], res])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de789be9",
   "metadata": {},
   "source": [
    "## Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aca4af12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ay4fx13l) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lilac-field-32</strong> at: <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/ay4fx13l' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/ay4fx13l</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230630_111142-ay4fx13l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ay4fx13l). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ec574de3234ad788a0b222fe2b1056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\smonn\\Desktop\\T-AIA-902-NAN_2\\notebooks\\simon\\wandb\\run-20230630_111144-nxc8nky4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/nxc8nky4' target=\"_blank\">curious-star-33</a></strong> to <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/nxc8nky4' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/nxc8nky4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'wandb.config = {\\n    \"learning_rate\": 0.001,\\n    \"batch_size\": 128,\\n    \"epsilon\": 1\\n}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_delay = 0.001\n",
    "target_update = 20 # update target network every 20 episode\n",
    "memory_size = 50000 # TODO try memory size and batch\n",
    "lr_initial = 0.001\n",
    "lr_final = 0.0001\n",
    "num_episodes = 3000\n",
    "gamma = 0.75\n",
    "gamma_lr = (lr_final / lr_initial) ** (1 / num_episodes)\n",
    "\n",
    "run = wandb.init(project=\"Taxi-Deep-Q-learning\", entity=os.getenv(\"WANDB_LOGIN\"))\n",
    "'''wandb.config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 128,\n",
    "    \"epsilon\": 1\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3646efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_delay)\n",
    "agent = Agent(env, strategy)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "policy_net = DQN(env).to(device)\n",
    "target_net = DQN(env).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval() # eval mode, not training\n",
    "\n",
    "optimizer = torch.optim.Adam(params=policy_net.parameters(), lr=lr_initial)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8b65f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode: 0/3000\r"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3384\\90750471.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#clear_output(wait=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training episode: {0}/{1}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mepisode_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;34m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_reset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\gym\\wrappers\\env_checker.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0menv_reset_passive_checker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\gym\\envs\\toy_text\\taxi.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"human\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"prob\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"action_mask\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\gym\\envs\\toy_text\\taxi.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_gui\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_render_gui\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\gym\\envs\\toy_text\\taxi.py\u001b[0m in \u001b[0;36m_render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    364\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m                 \u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackground_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb\"|\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34mb\"|\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian_vert\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "max_step = 100\n",
    "all_rewards = []\n",
    "epsilon_vec = []\n",
    "episode_durations = []\n",
    "for episode in range(num_episodes):\n",
    "    #clear_output(wait=True)\n",
    "    print(\"Training episode: {0}/{1}\".format(episode, num_episodes), end=\"\\r\")\n",
    "    state = env.reset()[0]\n",
    "    episode_reward = 0\n",
    "    for step in range(max_step):\n",
    "        # Do action\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        new_state, reward, done, info, _ = env.step(action)\n",
    "        memory.push(Experience(torch.tensor([state], device=device), \n",
    "                               torch.tensor([action], device=device),\n",
    "                               torch.tensor([new_state], device=device),\n",
    "                               torch.tensor([reward], device=device),\n",
    "                               torch.tensor([done], device=device, dtype=torch.bool)))\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "        # Train model\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states_b, actions_b, rewards_b, new_states_b, dones_b = extract_tensors(experiences)\n",
    "            \n",
    "            # predict q values\n",
    "            current_q_values = QValues.get_current(policy_net, states_b, actions_b)\n",
    "            \n",
    "            # expected q values\n",
    "            next_q_values = QValues.get_next(target_net, new_states_b)\n",
    "            \n",
    "            target_q_values = (next_q_values * gamma) + rewards_b # look better ?\n",
    "            \n",
    "            # compute loss\n",
    "            loss = nn.functional.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            \n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad() # prevent accumulating gradients during back props\n",
    "            loss.backward() # compute the gradient of the loss with respect of weight and biases in the policy net\n",
    "            for param in policy_net.parameters(): # Clips gradients computed during backpropagation to avoid explosion of gradients\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            optimizer.step() # update the weight and biases with the just previous gradients computed     \n",
    "        if done:\n",
    "            break\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict()) # update target net with policy net weight\n",
    "        \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    wandb.log({\"reward\": episode_reward, \"duration\": step, \"epsilon\": agent.get_epsilon(), \"learning_rate\": current_lr})\n",
    "    all_rewards.append(episode_reward)\n",
    "    episode_durations.append(step)\n",
    "    epsilon_vec.append(agent.get_epsilon())\n",
    "    agent.current_episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2431b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:nxc8nky4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb314c8bcf5c4a75bbdf7722e9d31bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>duration</td><td>███▅▄█▃▄▂▃▂▃▂▁▂▂▂▂▂▃▁▂▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epsilon</td><td>██▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>██▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>reward</td><td>▁▁▂▅▅▂█▆▇▆▇▇▇█▇▇▇▇█▇█▇▇█▇███████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>duration</td><td>15</td></tr><tr><td>epsilon</td><td>0.05934</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>reward</td><td>5</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">curious-star-33</strong> at: <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/nxc8nky4' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/nxc8nky4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230630_111144-nxc8nky4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:nxc8nky4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb4d1135de64ad9aa1a235122ab1513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666656966, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\smonn\\Desktop\\T-AIA-902-NAN_2\\notebooks\\simon\\wandb\\run-20230630_111913-5ztyzv3o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smonnier/Taxi-Q-learning-Evaluate/runs/5ztyzv3o' target=\"_blank\">serene-meadow-18</a></strong> to <a href='https://wandb.ai/smonnier/Taxi-Q-learning-Evaluate' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smonnier/Taxi-Q-learning-Evaluate' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Q-learning-Evaluate</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smonnier/Taxi-Q-learning-Evaluate/runs/5ztyzv3o' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Q-learning-Evaluate/runs/5ztyzv3o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a596b2b6bb624c7cbb247ea9282d500b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.009 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.155646…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Drop</td><td>▁▁▁▁▅▅▁▁▁▁▁▁▁██▁▁▁▁▁</td></tr><tr><td>Pickup</td><td>█▁▁▁▁▁▁██▁▁▁▁▁▁█▁▁▁▁</td></tr><tr><td>down</td><td>▅▃▄▃▅▇█▇▅▁▇▄▅▆▄▃▆▃▇▄</td></tr><tr><td>duration</td><td>█▅▅▁▇▅▆▇▅▁▇▇▅▇▅▅▄▅▆▅</td></tr><tr><td>left</td><td>█▄▁▁▇▅▇█▅▁██▇▅▅▄▇█▇█</td></tr><tr><td>penalties</td><td>▅▁▁▁▅▅▁▅▅▁▁▁▁██▅▁▁▁▁</td></tr><tr><td>reward</td><td>▂▆▆█▃▄▆▃▄█▅▅▆▁▂▄▇▆▆▆</td></tr><tr><td>right</td><td>█▇▅▁▅▄▁▅▅▂▇█▅▄▁█▁▅▅▄</td></tr><tr><td>up</td><td>▅▆█▅▅▁▅▁▃▅▁▅▁▅▆▅▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Drop</td><td>1</td></tr><tr><td>Pickup</td><td>1</td></tr><tr><td>down</td><td>3</td></tr><tr><td>duration</td><td>14</td></tr><tr><td>left</td><td>5</td></tr><tr><td>penalties</td><td>0</td></tr><tr><td>reward</td><td>7</td></tr><tr><td>right</td><td>2</td></tr><tr><td>up</td><td>2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">serene-meadow-18</strong> at: <a href='https://wandb.ai/smonnier/Taxi-Q-learning-Evaluate/runs/5ztyzv3o' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Q-learning-Evaluate/runs/5ztyzv3o</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230630_111913-5ztyzv3o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 20 episodes:\n",
      "Average timesteps per episode: 15.55\n",
      "Average penalties per episode: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Evaluate agent's performance after Q-learning\n",
    "DOWN = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "LEFT = 3\n",
    "PICKUP = 4\n",
    "DROP = 5\n",
    "\n",
    "epochs = 0\n",
    "\n",
    "run = wandb.init(project=\"Taxi-Q-learning-Evaluate\", entity=os.getenv(\"WANDB_LOGIN\"))\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 20\n",
    "\n",
    "for _ in range(episodes):\n",
    "    env = gym.make(\"Taxi-v3\", render_mode=\"human\").env\n",
    "    state = env.reset()[0]\n",
    "\n",
    "    epochs, penalties, reward, episode_reward = 0, 0, 0, 0\n",
    "    episode_action = [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        episode_action[action] += 1\n",
    "        state, reward, done, info, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "        env.render()\n",
    "    env.close()\n",
    "    wandb.log({\"reward\": episode_reward, \"duration\": epochs,\n",
    "               \"penalties\": penalties,\n",
    "               \"down\": episode_action[DOWN],\n",
    "               \"up\": episode_action[UP],\n",
    "               \"right\": episode_action[RIGHT],\n",
    "               \"left\": episode_action[LEFT],\n",
    "               \"Pickup\": episode_action[PICKUP],\n",
    "               \"Drop\": episode_action[DROP]})\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "run.finish()\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f9428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL (Gym)",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
