{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139ca77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b2babe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: smonnier. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\smonn\\Desktop\\T-AIA-902-NAN_2\\notebooks\\simon\\wandb\\run-20230630_170315-mbye4xwl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/mbye4xwl' target=\"_blank\">clean-frog-62</a></strong> to <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/mbye4xwl' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/mbye4xwl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/mbye4xwl?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x22ead427d30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init Weights and Biases\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"Taxi-Deep-Q-learning\", entity=os.getenv(\"WANDB_LOGIN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f9c3667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: 500 11 7\n",
      "Action: 6\n"
     ]
    }
   ],
   "source": [
    "env_name = \"Taxi-v3\"\n",
    "env = gym.make(env_name)\n",
    "print('Observation:', env.observation_space.n, env.desc.shape[1], env.desc.shape[0])\n",
    "print('Action:', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4efb0",
   "metadata": {},
   "source": [
    "## Deep QNetwork\n",
    "nn.module = base for all neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40832a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        # init dqn layer\n",
    "        self.emb = nn.Embedding(500, 10)\n",
    "        self.fc1 = nn.Linear(in_features=10, out_features=50)\n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=50)\n",
    "        self.out_features = nn.Linear(in_features=50, out_features=env.action_space.n)\n",
    "    \n",
    "    def forward(self, input_t):\n",
    "        #print(input_t)\n",
    "        input_t = nn.functional.relu(self.fc1(self.emb(input_t)))\n",
    "        #print(input_t)\n",
    "        #input_t = nn.functional.relu(self.fc1(input_t))\n",
    "        input_t = nn.functional.relu(self.fc2(input_t))\n",
    "        input_t = self.out_features(input_t)\n",
    "        return input_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ea4dc",
   "metadata": {},
   "source": [
    "## Experience class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b5c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', ('state', 'action', 'next_state', 'reward', 'done'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba14a40",
   "metadata": {},
   "source": [
    "## Replay Memory class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa0c3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea294113",
   "metadata": {},
   "source": [
    "## Epsilon Greedy Strategy class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4719bbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "    \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * (current_step) * self.decay) # TODO fix it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7e1811",
   "metadata": {},
   "source": [
    "## Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b563b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, strategy): # TODO add device for torch\n",
    "        self.current_episode = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        \n",
    "    def get_epsilon(self):\n",
    "        epsilon = self.strategy.get_exploration_rate(self.current_episode)\n",
    "        return epsilon\n",
    "    \n",
    "    def select_action(self, state, policy_network):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_episode)\n",
    "        #print(rate)\n",
    "        # random.uniform(0, 1)?\n",
    "        if rate > random.random():\n",
    "            return random.randrange(self.num_actions) # explore\n",
    "        else:\n",
    "            with torch.no_grad(): # No grad because we use the model to select an action and not for training\n",
    "                predicted = policy_network(torch.tensor([state], device=self.device)) # TODO check diff no tensor\n",
    "                return predicted.argmax(dim=1).item() # exploit    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e145d2b7",
   "metadata": {},
   "source": [
    "## Usefull function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb639373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    batch = Experience(*zip(*experiences))\n",
    "    tensor_state = torch.cat(batch.state)\n",
    "    tensor_action = torch.cat(batch.action)\n",
    "    tensor_reward = torch.cat(batch.reward)\n",
    "    tensor_next_state = torch.cat(batch.next_state)\n",
    "    tensor_done = torch.cat(batch.done)\n",
    "    return (tensor_state, tensor_action, tensor_reward, tensor_next_state, tensor_done)\n",
    "\n",
    "#https://www.youtube.com/watch?v=ewRw996uevM&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=18 15.00min\n",
    "class QValues():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(1)) # send all states and actions pairs and get list of qvalues\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_next(target_net, next_states):\n",
    "        values = target_net(next_states).max(dim=1)[0]\n",
    "        return values\n",
    "        \n",
    "def get_game_state(env):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    taxi_row, taxi_col, pass_idx, dest_idx = env.decode(env.s)\n",
    "    rend = env.desc.copy()\n",
    "    if pass_idx < 4:\n",
    "        rend[1 + env.locs[pass_idx][0]][2 * env.locs[pass_idx][1] + 1] = 'P'\n",
    "    rend[1 + env.locs[dest_idx][0]][2 * env.locs[dest_idx][1] + 1] = 'D'\n",
    "    rend[1 + taxi_row][2 * taxi_col + 1] = 'T'\n",
    "    rend = rend.view(np.uint8).astype(np.float32) # Char to float\n",
    "    rend = torch.tensor(rend, device=device) # numpy to tensor\n",
    "    #print(rend.unsqueeze(1))\n",
    "    return rend.unsqueeze(0)\n",
    "\n",
    "def moving_average(x, periods=5):\n",
    "        if len(x) < periods:\n",
    "            return x\n",
    "        cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "        res = (cumsum[periods:] - cumsum[:-periods]) / periods\n",
    "        return np.hstack([x[:periods-1], res])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de789be9",
   "metadata": {},
   "source": [
    "## Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aca4af12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:mbye4xwl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-frog-62</strong> at: <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/mbye4xwl' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/mbye4xwl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230630_170315-mbye4xwl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:mbye4xwl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3951c67c690c4217b1fb6fdd947e11e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\smonn\\Desktop\\T-AIA-902-NAN_2\\notebooks\\simon\\wandb\\run-20230630_170316-hv6no3pp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/hv6no3pp' target=\"_blank\">crimson-fire-63</a></strong> to <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/hv6no3pp' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/hv6no3pp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'wandb.config = {\\n    \"learning_rate\": 0.001,\\n    \"batch_size\": 128,\\n    \"epsilon\": 1\\n}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_delay = 0.001\n",
    "target_update = 20 # update target network every 20 episode\n",
    "memory_size = 50000 # TODO try memory size and batch\n",
    "lr_initial = 0.001\n",
    "lr_final = 0.0001\n",
    "num_episodes = 3000\n",
    "gamma = 0.85\n",
    "gamma_lr = (lr_final / lr_initial) ** (1 / num_episodes)\n",
    "\n",
    "run = wandb.init(project=\"Taxi-Deep-Q-learning\", entity=os.getenv(\"WANDB_LOGIN\"))\n",
    "'''wandb.config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 128,\n",
    "    \"epsilon\": 1\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3646efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_delay)\n",
    "agent = Agent(env, strategy)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "policy_net = DQN(env).to(device)\n",
    "target_net = DQN(env).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval() # eval mode, not training\n",
    "\n",
    "optimizer = torch.optim.Adam(params=policy_net.parameters(), lr=lr_initial)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8b65f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode: 2999/3000\r"
     ]
    }
   ],
   "source": [
    "max_step = 100\n",
    "all_rewards = []\n",
    "epsilon_vec = []\n",
    "episode_durations = []\n",
    "for episode in range(num_episodes):\n",
    "    #clear_output(wait=True)\n",
    "    print(\"Training episode: {0}/{1}\".format(episode, num_episodes), end=\"\\r\")\n",
    "    state = env.reset()[0]\n",
    "    episode_reward = 0\n",
    "    for step in range(max_step):\n",
    "        # Do action\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        new_state, reward, done, info, _ = env.step(action)\n",
    "        memory.push(Experience(torch.tensor([state], device=device), \n",
    "                               torch.tensor([action], device=device),\n",
    "                               torch.tensor([new_state], device=device),\n",
    "                               torch.tensor([reward], device=device),\n",
    "                               torch.tensor([done], device=device, dtype=torch.bool)))\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "        # Train model\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states_b, actions_b, rewards_b, new_states_b, dones_b = extract_tensors(experiences)\n",
    "            \n",
    "            # predict q values\n",
    "            current_q_values = QValues.get_current(policy_net, states_b, actions_b)\n",
    "            \n",
    "            # expected q values\n",
    "            next_q_values = QValues.get_next(target_net, new_states_b)\n",
    "            \n",
    "            target_q_values = (next_q_values * gamma) + rewards_b # look better ?\n",
    "            \n",
    "            # compute loss\n",
    "            loss = nn.functional.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            \n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad() # prevent accumulating gradients during back props\n",
    "            loss.backward() # compute the gradient of the loss with respect of weight and biases in the policy net\n",
    "            for param in policy_net.parameters(): # Clips gradients computed during backpropagation to avoid explosion of gradients\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            optimizer.step() # update the weight and biases with the just previous gradients computed     \n",
    "        if done:\n",
    "            break\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict()) # update target net with policy net weight\n",
    "        \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    wandb.log({\"reward\": episode_reward, \"duration\": step, \"epsilon\": agent.get_epsilon(), \"learning_rate\": current_lr})\n",
    "    all_rewards.append(episode_reward)\n",
    "    episode_durations.append(step)\n",
    "    epsilon_vec.append(agent.get_epsilon())\n",
    "    agent.current_episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2431b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:hv6no3pp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>duration</td><td>████████▄▃██▃▂▁▂▃▂▂▁▂▂▂▂▁▂▂▂▂▂▂▂▁▁▂▁▂▂▁▁</td></tr><tr><td>epsilon</td><td>██▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>██▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>reward</td><td>▁▁▂▁▂▃▄▃▅▆▃▃▇▇██▇█▇█▇█████▇▇████████▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>duration</td><td>9</td></tr><tr><td>epsilon</td><td>0.05934</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>reward</td><td>11</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crimson-fire-63</strong> at: <a href='https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/hv6no3pp' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Deep-Q-learning/runs/hv6no3pp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230630_170316-hv6no3pp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:hv6no3pp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c03e8567605440ca253e54b5612839a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\smonn\\Desktop\\T-AIA-902-NAN_2\\notebooks\\simon\\wandb\\run-20230630_171148-g3hp0hzq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smonnier/Taxi-Q-learning-Evaluate/runs/g3hp0hzq' target=\"_blank\">eternal-spaceship-24</a></strong> to <a href='https://wandb.ai/smonnier/Taxi-Q-learning-Evaluate' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smonnier/Taxi-Q-learning-Evaluate' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Q-learning-Evaluate</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smonnier/Taxi-Q-learning-Evaluate/runs/g3hp0hzq' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Q-learning-Evaluate/runs/g3hp0hzq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Drop</td><td>█▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁</td></tr><tr><td>Pickup</td><td>▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█</td></tr><tr><td>down</td><td>▆▅▅▅▅▅▆▅▁▃▁▅█▅▅▅▃▅█▅</td></tr><tr><td>duration</td><td>▅▃▅▁▂▅▂▅▂▂▃▅▅█▄▄▂▃▅▄</td></tr><tr><td>left</td><td>▅▃▆▃▃█▁██▆▃▆▅▅▅█▆█▆█</td></tr><tr><td>penalties</td><td>█▁█▁▁▁▁▁█▁▁▁▁▁▁▁▁▁██</td></tr><tr><td>reward</td><td>▁▇▁█▇▅▇▅▄▇▇▅▅▃▆▆▇▇▁▂</td></tr><tr><td>right</td><td>▅▅▅▁▅▇▅▅▁▁▇▅▇█▅▂▁▂▅▄</td></tr><tr><td>up</td><td>▂▂▃▂▁▂▁▂▁▃▃▃▁█▂▂▃▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Drop</td><td>1</td></tr><tr><td>Pickup</td><td>2</td></tr><tr><td>down</td><td>4</td></tr><tr><td>duration</td><td>15</td></tr><tr><td>left</td><td>4</td></tr><tr><td>penalties</td><td>1</td></tr><tr><td>reward</td><td>-3</td></tr><tr><td>right</td><td>2</td></tr><tr><td>up</td><td>2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eternal-spaceship-24</strong> at: <a href='https://wandb.ai/smonnier/Taxi-Q-learning-Evaluate/runs/g3hp0hzq' target=\"_blank\">https://wandb.ai/smonnier/Taxi-Q-learning-Evaluate/runs/g3hp0hzq</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230630_171148-g3hp0hzq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 20 episodes:\n",
      "Average timesteps per episode: 14.35\n",
      "Average penalties per episode: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Evaluate agent's performance after Q-learning\n",
    "DOWN = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "LEFT = 3\n",
    "PICKUP = 4\n",
    "DROP = 5\n",
    "\n",
    "epochs = 0\n",
    "\n",
    "run = wandb.init(project=\"Taxi-Q-learning-Evaluate\", entity=os.getenv(\"WANDB_LOGIN\"))\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 20\n",
    "\n",
    "for _ in range(episodes):\n",
    "    env = gym.make(\"Taxi-v3\", render_mode=\"human\").env\n",
    "    state = env.reset()[0]\n",
    "\n",
    "    epochs, penalties, reward, episode_reward = 0, 0, 0, 0\n",
    "    episode_action = [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        episode_action[action] += 1\n",
    "        state, reward, done, info, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "        env.render()\n",
    "    env.close()\n",
    "    wandb.log({\"reward\": episode_reward, \"duration\": epochs,\n",
    "               \"penalties\": penalties,\n",
    "               \"down\": episode_action[DOWN],\n",
    "               \"up\": episode_action[UP],\n",
    "               \"right\": episode_action[RIGHT],\n",
    "               \"left\": episode_action[LEFT],\n",
    "               \"Pickup\": episode_action[PICKUP],\n",
    "               \"Drop\": episode_action[DROP]})\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "run.finish()\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f9428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL (Gym)",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
